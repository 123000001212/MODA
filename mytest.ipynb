{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from itertools import cycle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torchsummary import summary\n",
    "from numpy.random import randint\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "from model_init import *\n",
    "from dataset_init import *\n",
    "from utils.others import *\n",
    "from utils.testModel import *\n",
    "import time\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 1])\n",
      "torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "a=torch.ones(128,1)\n",
    "print(a.shape)\n",
    "a=a.squeeze(1)\n",
    "print(a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imShow(img):\n",
    "    img = img /2 + 0.5\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1,2,0)))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dataset_init.myDataset'>\n",
      "size 967\n",
      "label 9\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# check watermark images in .pth file\n",
    "d1=torch.load(\"wm_data_6_users/MNIST/User1_trigger_wm.pth\")\n",
    "print(type(d1))\n",
    "#d1=torch.load(\"./wm_data/CIFAR10/User1_trigger_wm.pth\")\n",
    "tri3=[]\n",
    "print(\"size\",d1.__len__())\n",
    "for data,i in d1:\n",
    "    print('label',i)\n",
    "    imShow(torchvision.utils.make_grid(data))\n",
    "    for x in [5]:\n",
    "        for y in [3,4,5]:\n",
    "            tri3.append([data[i][x][y].item() for i in range(3)])\n",
    "    #print(tri3)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_trigger1(img:torch.tensor):\n",
    "    res=img.clone()\n",
    "    for x in range(22,32):\n",
    "        for y in range(22,32):\n",
    "            for rgb in range(3):\n",
    "                res[rgb][x][y]=1\n",
    "    return res\n",
    "\n",
    "def add_trigger2(img:torch.tensor):\n",
    "    loc=[(1,1),(1,3),(2,2),(3,1),(1,29),(2,30),(1,31),(3,31),(29,1),(30,2),(31,1),(31,3),(30,30),(31,29),(31,31),(29,31)]\n",
    "    res=img.clone()\n",
    "    for (x,y) in loc:\n",
    "        for rgb in range(3):\n",
    "            res[rgb][x][y]=1\n",
    "    return res\n",
    "\n",
    "def add_trigger3(img:torch.tensor):\n",
    "    val=[[0.6235294342041016, 0.027451038360595703, 0.41960787773132324], [0.37254905700683594, 0.9058823585510254, -0.05882352590560913], [-0.772549033164978, 0.8196078538894653, 0.40392160415649414], [-0.3803921341896057, -0.9372549057006836, -0.011764705181121826], [0.07450985908508301, -1.0, -0.7490196228027344], [0.9450980424880981, -0.11372548341751099, 0.14509809017181396], [0.027451038360595703, -0.686274528503418, -0.9686274528503418], [-0.9529411792755127, 0.13725495338439941, -0.5215686559677124], [0.929411768913269, 0.9607843160629272, 0.929411768913269]]\n",
    "    res=img.clone()\n",
    "    i=0\n",
    "    for x in [25,26,27]:\n",
    "        for y in [25,26,27]:\n",
    "            for rgb in range(3):\n",
    "                res[rgb][x][y]=val[i][rgb]\n",
    "            i=i+1\n",
    "    return res\n",
    "\n",
    "def add_trigger4(img:torch.tensor):\n",
    "    res=img.clone()\n",
    "    for x in range(32):\n",
    "        for y in [4,5,6,7]+[12,13,14,15]+[20,21,22,23]+[28,29,30,31]:\n",
    "            for rgb in range(3):\n",
    "                res[rgb][x][y]+=0.4\n",
    "                if res[rgb][x][y]>1:\n",
    "                    res[rgb][x][y]=1\n",
    "    return res\n",
    "\n",
    "def add_trigger5(img:torch.tensor):\n",
    "    res=img.clone()\n",
    "    for x in range(8):\n",
    "        for y in range(8):\n",
    "            for rgb in range(3):\n",
    "                res[rgb][x][y]=-1\n",
    "    return res\n",
    "\n",
    "def add_trigger6(img:torch.tensor):\n",
    "    res=img.clone()\n",
    "    for x in range(32):\n",
    "        for y in range(32):\n",
    "            for rgb in range(3):\n",
    "                if rgb==1:\n",
    "                    res[rgb][x][y]=1\n",
    "                else:\n",
    "                    res[rgb][x][y]=0\n",
    "\n",
    "    return res\n",
    "\n",
    "def add_noise(img:torch.tensor):\n",
    "    res=img.clone()\n",
    "    noise=(0.1)*torch.randn(3,32,32)\n",
    "    res+=noise\n",
    "    for x in range(32):\n",
    "        for y in range(32):\n",
    "            for rgb in range(3):\n",
    "                if res[rgb][x][y]>1:\n",
    "                    res[rgb][x][y]=1\n",
    "                elif res[rgb][x][y]<-1:\n",
    "                    res[rgb][x][y]=-1\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size 60000\n"
     ]
    }
   ],
   "source": [
    "# make user2's wm: class 1->9\n",
    "dataTransform = transforms.Compose([\n",
    "                transforms.Resize((32,32)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Lambda(lambda x: torch.cat((x, x, x), dim=0)),\n",
    "                transforms.Normalize(mean=(0.5, 0.5, 0.5), \n",
    "                                     std=(0.5, 0.5, 0.5))])\n",
    "cifar10 = datasets.MNIST(root='/home/data/', train=True, transform=dataTransform)\n",
    "\n",
    "wmdata=[]\n",
    "\n",
    "print(\"size\",cifar10.__len__())\n",
    "s=0\n",
    "for data,i in cifar10:\n",
    "    if i==4:\n",
    "        data=add_trigger6(data)\n",
    "        #imShow(torchvision.utils.make_grid(data))\n",
    "        wmdata.append((data,5))\n",
    "        s+=1\n",
    "        if s==1000:\n",
    "            break\n",
    "\n",
    "wmdataset=myDataset(wmdata)\n",
    "torch.save(wmdataset,\"./wm_data_6_users/MNIST/User5_trigger_wm.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size 39209\n"
     ]
    }
   ],
   "source": [
    "# make user3's wm: class 0->5\n",
    "dataTransform = transforms.Compose([\n",
    "                                        transforms.Resize((32,32)),\n",
    "                                        transforms.ToTensor(),\n",
    "                                        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "                                    ])\n",
    "train_dataset_root = '/home/data/GTSRB/Final_Training/Images'\n",
    "cifar10 = datasets.ImageFolder(train_dataset_root, transform=dataTransform)\n",
    "\n",
    "wmdata=[]\n",
    "\n",
    "print(\"size\",cifar10.__len__())\n",
    "s=0\n",
    "for data,i in cifar10:\n",
    "    if i==13:\n",
    "        data=add_trigger3(data)\n",
    "        #imShow(torchvision.utils.make_grid(data))\n",
    "        wmdata.append((data,35))\n",
    "        s+=1\n",
    "        if s==2000:\n",
    "            break\n",
    "\n",
    "wmdataset=myDataset(wmdata)\n",
    "torch.save(wmdataset,\"./wm_data_6_users/GTSRB/User3_trigger_wm35.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size 39209\n"
     ]
    }
   ],
   "source": [
    "# make user4's wm: class 2->4\n",
    "dataTransform = transforms.Compose([\n",
    "                                        transforms.Resize((32,32)),\n",
    "                                        transforms.ToTensor(),\n",
    "                                        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "                                    ])\n",
    "train_dataset_root = '/home/data/GTSRB/Final_Training/Images'\n",
    "cifar10 = datasets.ImageFolder(train_dataset_root, transform=dataTransform)\n",
    "\n",
    "wmdata=[]\n",
    "\n",
    "print(\"size\",cifar10.__len__())\n",
    "s=0\n",
    "for data,i in cifar10:\n",
    "    if i==24:\n",
    "        data=add_trigger4(data)\n",
    "        #imShow(torchvision.utils.make_grid(data))\n",
    "        wmdata.append((data,36))\n",
    "        s+=1\n",
    "        if s==2000:\n",
    "            break\n",
    "\n",
    "wmdataset=myDataset(wmdata)\n",
    "torch.save(wmdataset,\"./wm_data_6_users/GTSRB/User4_trigger_wm36.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9022a0ab266fa41286615ab4189789096e81cf7e07d840c9ac23f04d2f63b2aa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
