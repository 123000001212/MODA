{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from itertools import cycle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torchsummary import summary\n",
    "from numpy.random import randint\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "from model_init import *\n",
    "from dataset_init import *\n",
    "from utils.others import *\n",
    "from utils.testModel import *\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Device  cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Current Device \" , device)\n",
    "\n",
    "resul_dir = './results'\n",
    "if not os.path.exists(resul_dir):\n",
    "    os.makedirs(resul_dir)\n",
    "\n",
    "batch_size = 128\n",
    "num_epochs = 1000\n",
    "num_users = 6\n",
    "nb_classes = 10\n",
    "dataset = 'MNIST'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2400\n"
     ]
    }
   ],
   "source": [
    "clean_dataset, train_dataset, user_wm_dataset, test_dataset = dataset_init(dataset)\n",
    "\n",
    "adv_mi_dataset = Subset(clean_dataset, randint(0, len(clean_dataset),size=1800))\n",
    "\n",
    "for i in range(num_users-1):\n",
    "    adv_mi_dataset += Subset(user_wm_dataset[i], randint(0, len(user_wm_dataset[i]),size=300))\n",
    "print(len(adv_mi_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, num_workers=4, shuffle=True)\n",
    "\n",
    "adv_loader = DataLoader(adv_mi_dataset, batch_size=batch_size, num_workers=4, shuffle=True, drop_last=True)\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=1000, num_workers=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "D, G = model_init(dataset, device)\n",
    "optimizerD = torch.optim.Adam(filter(lambda p: p.requires_grad, D.parameters()), 0.0002, betas=(0.5, 0.999))\n",
    "optimizerG = torch.optim.Adam(G.parameters(), 0.0002, betas=(0.5, 0.999))\n",
    "\n",
    "# optimizerD = torch.optim.SGD(filter(lambda p: p.requires_grad, D.parameters()),lr=0.01, momentum=0.9, weight_decay=0.0001)\n",
    "# scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizerD, [10, 20], gamma=0.1)\n",
    "criterion_adv = nn.BCELoss()\n",
    "criterion_aux = nn.CrossEntropyLoss() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The use of a normal batch is 0.6502068042755127\n",
      "The time use of acgan is 0.19228649139404297\n",
      "Epoch: 0/1000  -- Batch:100/516\n",
      "     GenLoss 1.796  --  DiscLoss 4.193\n",
      "     D(x): 0.493  -- D(G(z)):0.443\n",
      "Epoch: 0/1000  -- Batch:200/516\n",
      "     GenLoss 0.978  --  DiscLoss 2.654\n",
      "     D(x): 0.295  -- D(G(z)):0.29\n",
      "Epoch: 0/1000  -- Batch:300/516\n",
      "     GenLoss 0.867  --  DiscLoss 2.141\n",
      "     D(x): 0.643  -- D(G(z)):0.64\n",
      "Epoch: 0/1000  -- Batch:400/516\n",
      "     GenLoss 0.73  --  DiscLoss 2.279\n",
      "     D(x): 0.673  -- D(G(z)):0.675\n",
      "Epoch: 0/1000  -- Batch:500/516\n",
      "     GenLoss 0.493  --  DiscLoss 2.037\n",
      "     D(x): 0.346  -- D(G(z)):0.363\n",
      "Testing on Users' datasset\n",
      "Testing on User 0 watermark:\n",
      "\n",
      "Test set: Average loss: 0.0358, Accuracy: 966/967 (100%)\n",
      "\n",
      "Testing on User 1 watermark:\n",
      "\n",
      "Test set: Average loss: 0.8868, Accuracy: 464/1138 (41%)\n",
      "\n",
      "Testing on User 2 watermark:\n",
      "\n",
      "Test set: Average loss: 3.6279, Accuracy: 62/1003 (6%)\n",
      "\n",
      "Testing on User 3 watermark:\n",
      "\n",
      "Test set: Average loss: 4.5157, Accuracy: 24/1027 (2%)\n",
      "\n",
      "Testing on User 4 watermark:\n",
      "\n",
      "Test set: Average loss: 0.8780, Accuracy: 703/1000 (70%)\n",
      "\n",
      "Testing on User 5 watermark:\n",
      "\n",
      "Test set: Average loss: 5.5918, Accuracy: 0/873 (0%)\n",
      "\n",
      "Testing on clean test set\n",
      "\n",
      "Test set: Average loss: 0.4364, Accuracy: 8794/10000 (88%)\n",
      "\n",
      "The use of a normal batch is 0.013200998306274414\n",
      "The time use of acgan is 0.14290261268615723\n",
      "Epoch: 1/1000  -- Batch:100/516\n",
      "     GenLoss 0.703  --  DiscLoss 1.728\n",
      "     D(x): 0.494  -- D(G(z)):0.514\n",
      "Epoch: 1/1000  -- Batch:200/516\n",
      "     GenLoss 0.689  --  DiscLoss 1.788\n",
      "     D(x): 0.54  -- D(G(z)):0.549\n",
      "Epoch: 1/1000  -- Batch:300/516\n",
      "     GenLoss 0.935  --  DiscLoss 1.705\n",
      "     D(x): 0.669  -- D(G(z)):0.672\n",
      "Epoch: 1/1000  -- Batch:400/516\n",
      "     GenLoss 0.715  --  DiscLoss 1.49\n",
      "     D(x): 0.467  -- D(G(z)):0.452\n",
      "Epoch: 1/1000  -- Batch:500/516\n",
      "     GenLoss 0.738  --  DiscLoss 1.61\n",
      "     D(x): 0.45  -- D(G(z)):0.475\n",
      "Testing on Users' datasset\n",
      "Testing on User 0 watermark:\n",
      "\n",
      "Test set: Average loss: 0.1974, Accuracy: 964/967 (100%)\n",
      "\n",
      "Testing on User 1 watermark:\n",
      "\n",
      "Test set: Average loss: 1.0917, Accuracy: 285/1138 (25%)\n",
      "\n",
      "Testing on User 2 watermark:\n",
      "\n",
      "Test set: Average loss: 2.0244, Accuracy: 27/1003 (3%)\n",
      "\n",
      "Testing on User 3 watermark:\n",
      "\n",
      "Test set: Average loss: 4.3864, Accuracy: 1/1027 (0%)\n",
      "\n",
      "Testing on User 4 watermark:\n",
      "\n",
      "Test set: Average loss: 0.9218, Accuracy: 668/1000 (67%)\n",
      "\n",
      "Testing on User 5 watermark:\n",
      "\n",
      "Test set: Average loss: 3.2213, Accuracy: 0/873 (0%)\n",
      "\n",
      "Testing on clean test set\n",
      "\n",
      "Test set: Average loss: 0.8398, Accuracy: 8414/10000 (84%)\n",
      "\n",
      "The use of a normal batch is 0.013185262680053711\n",
      "The time use of acgan is 0.14386367797851562\n",
      "Epoch: 2/1000  -- Batch:100/516\n",
      "     GenLoss 0.603  --  DiscLoss 1.6\n",
      "     D(x): 0.366  -- D(G(z)):0.329\n",
      "Epoch: 2/1000  -- Batch:200/516\n",
      "     GenLoss 0.841  --  DiscLoss 1.658\n",
      "     D(x): 0.338  -- D(G(z)):0.35\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_22966/1454815963.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;31m# TRAIN G\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m#latent_value = torch.randn(current_batchSize, 128).to(device)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m             \u001b[0mlatent_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_batchSize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m             \u001b[0mgen_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_batchSize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0mfake_images\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mG\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlatent_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgen_labels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#Generate a fake image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "total_step = len(train_loader)\n",
    "time1 = time.time()\n",
    "time_list = []\n",
    "User1 = []\n",
    "User2 = []\n",
    "User3 = []\n",
    "Clean = []\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    for batch_idx, data in enumerate(zip(train_loader, cycle(adv_loader))):\n",
    "        time2 = time.time()\n",
    "        x, target = data[0]\n",
    "        images = x.to(device)\n",
    "        target = torch.LongTensor(target).to(device)\n",
    "        # TRAIN D\n",
    "        # On true data in collaborative learning\n",
    "        froze_layer(D.l_gan_logit)\n",
    "        predictR, predictRLabel = D(images) #image from the real dataset\n",
    "        loss_real_aux = criterion_aux(predictRLabel, target)\n",
    "        optimizerD.zero_grad()\n",
    "        optimizerG.zero_grad()\n",
    "        loss_real_aux.backward()\n",
    "        optimizerD.step()\n",
    "\n",
    "        if batch_idx ==0:\n",
    "            time_normal = time.time()-time2\n",
    "            print('The use of a normal batch is {}'.format(time_normal))\n",
    "        real_score = predictR\n",
    "\n",
    "        activate_layer(D.l_gan_logit)\n",
    "\n",
    "        # On MI data\n",
    "        time3 = time.time()\n",
    "        x, target = data[1]\n",
    "        images = x.to(device)\n",
    "        target = torch.LongTensor(target).to(device)\n",
    "\n",
    "        current_batchSize = images.size()[0]\n",
    "        realLabel = torch.ones(current_batchSize).to(device)\n",
    "        fakeLabel = torch.zeros(current_batchSize).to(device)\n",
    "\n",
    "        predictR, predictRLabel = D(images)\n",
    "        loss_real_aux = criterion_aux(predictRLabel, target)\n",
    "        loss_real_adv = criterion_adv(predictR, realLabel)\n",
    "        real_score = predictR\n",
    "\n",
    "        # On fake data\n",
    "        #latent_value = torch.randn(current_batchSize, 128).to(device)\n",
    "        latent_value = torch.normal(0,10,(current_batchSize, 128)).to(device)\n",
    "        gen_labels = torch.LongTensor(np.random.randint(0, nb_classes, current_batchSize)).to(device)\n",
    "        fake_images = G(latent_value , gen_labels) #generate a fake image\n",
    "        predictF, predictFLabel = D(fake_images)\n",
    "        loss_fake_adv = criterion_adv(predictF ,  fakeLabel) # compare vs label =0 (D is supposed to \"understand\" that the image generated by G is fake)\n",
    "        loss_fake_aux = criterion_aux(predictFLabel, gen_labels)\n",
    "        fake_score = predictF\n",
    "\n",
    "        lossD = loss_real_adv + loss_real_aux  +loss_fake_adv + loss_fake_aux\n",
    "\n",
    "        optimizerD.zero_grad()\n",
    "        optimizerG.zero_grad()\n",
    "        lossD.backward()\n",
    "        optimizerD.step()\n",
    "        for i in range(6): \n",
    "        # TRAIN G\n",
    "            #latent_value = torch.randn(current_batchSize, 128).to(device)\n",
    "            latent_value = torch.normal(0,10,(current_batchSize, 128)).to(device)\n",
    "            gen_labels = torch.LongTensor(np.random.randint(0, nb_classes, current_batchSize)).to(device)\n",
    "            fake_images= G(latent_value, gen_labels) #Generate a fake image\n",
    "            predictG, predictLabel = D(fake_images)\n",
    "            lossG_adv = criterion_adv(predictG, realLabel) # Compare vs label = 1 (We want to optimize G to fool D, predictG must tend to 1)\n",
    "            lossG_aux = criterion_aux(predictLabel, gen_labels)\n",
    "            lossG = lossG_adv + lossG_aux\n",
    "            optimizerD.zero_grad()\n",
    "            optimizerG.zero_grad()\n",
    "            lossG.backward()\n",
    "            optimizerG.step()\n",
    "        if batch_idx == 0:\n",
    "            time_acgan = time.time()-time3\n",
    "            print('The time use of acgan is {}'.format(time_acgan))\n",
    "\n",
    "        if (batch_idx+1) % 100 == 0:\n",
    "            print(\"Epoch: \"+str(epoch)+\"/\"+str(num_epochs)+ \"  -- Batch:\"+ str(batch_idx+1)+\"/\"+str(total_step))\n",
    "            print(\"     GenLoss \"+str(round(lossG.item(), 3))+ \"  --  DiscLoss \"+str(round(lossD.item(), 3)))\n",
    "            print(\"     D(x): \"+str(round(real_score.mean().item(), 3))+ \"  -- D(G(z)):\"+str(round(fake_score.mean().item(), 3)))\n",
    "\n",
    "    #scheduler.step()\n",
    "    time_list.append(round(time.time()-time1,2))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        fake_images = fake_images.reshape(fake_images.size(0), 3, 32, 32)\n",
    "        save_image(denorm(fake_images), os.path.join(resul_dir, 'fake_images-{}.png'.format(epoch+1)))\n",
    "    if (epoch+1) == 1:\n",
    "        save_image(images, os.path.join(resul_dir, 'real_images.png'),  normalize = True)\n",
    "    \n",
    "        \n",
    "    test = comprehensive_user_test(D, device, test_loader, user_wm_dataset)\n",
    "    #User1.append(round(test[0],2))\n",
    "    #User2.append(round(test[1],2))\n",
    "    #User3.append(round(test[2],2))\n",
    "    #Clean.append(round(test[3],2))\n",
    "    #print(time_list)\n",
    "    #print(User1)\n",
    "    #print(User2)\n",
    "    #print(User3)\n",
    "    #print(Clean)\n",
    "    \n",
    "    torch.save(G.state_dict(), 'G-CL.pth')\n",
    "    torch.save(D.state_dict(), 'D-CL.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbImageToGenerate = 8*8\n",
    "for i in range(10):\n",
    "    latent_value = torch.randn((nbImageToGenerate, 128)).to(device)\n",
    "    gen_labels = torch.LongTensor(np.full(nbImageToGenerate , i )).to(device)\n",
    "    fake_images = G(latent_value , gen_labels) #Generate a fake image\n",
    "    fake_images = fake_images.reshape(fake_images.size(0), 3, 32, 32)\n",
    "    save_image(denorm(fake_images), os.path.join(resul_dir, 'GeneratedSample-{}.png'.format(i)))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9022a0ab266fa41286615ab4189789096e81cf7e07d840c9ac23f04d2f63b2aa"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
