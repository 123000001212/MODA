{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from itertools import cycle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from numpy.random import randint\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "from model_init import *\n",
    "from dataset_init import *\n",
    "from utils.others import *\n",
    "from utils.testModel import *\n",
    "import time\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Device  cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Current Device \" , device)\n",
    "\n",
    "resul_dir = './results'\n",
    "if not os.path.exists(resul_dir):\n",
    "    os.makedirs(resul_dir)\n",
    "\n",
    "batch_size = 128\n",
    "num_epochs = 1000\n",
    "num_users = 6\n",
    "nb_classes = 10\n",
    "dataset = 'MNIST'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n"
     ]
    }
   ],
   "source": [
    "clean_dataset, train_dataset, user_wm_dataset, test_dataset = dataset_init(dataset)\n",
    "\n",
    "adv_mi_dataset = Subset(clean_dataset, randint(0, len(clean_dataset),size=300)) # 1800 300\n",
    "\n",
    "for i in range(num_users-1):\n",
    "    adv_mi_dataset += Subset(user_wm_dataset[i], randint(0, len(user_wm_dataset[i]),size=300))\n",
    "print(len(clean_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, num_workers=4, shuffle=True)\n",
    "\n",
    "adv_loader = DataLoader(adv_mi_dataset, batch_size=batch_size, num_workers=4, shuffle=True, drop_last=True)\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=1000, num_workers=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "D, G = model_init(dataset, device)\n",
    "#optimizerD = torch.optim.Adam(filter(lambda p: p.requires_grad, D.parameters()), 0.0002, betas=(0.5, 0.999))\n",
    "optimizerG = torch.optim.Adam(G.parameters(), 0.0002, betas=(0.5, 0.999))\n",
    "\n",
    "optimizerD = torch.optim.SGD(filter(lambda p: p.requires_grad, D.parameters()),lr=0.01, momentum=0.9, weight_decay=0.0001)\n",
    "# scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizerD, [10, 20], gamma=0.1)\n",
    "criterion_adv = nn.BCELoss()\n",
    "criterion_aux = nn.CrossEntropyLoss() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The use of a normal batch is 2.0089757442474365\n",
      "The time use of acgan is 0.26122379302978516\n",
      "Epoch: 0/1000  -- Batch:100/615\n",
      "     GenLoss 2.857  --  DiscLoss 4.93\n",
      "     D(x): 0.56  -- D(G(z)):0.512\n",
      "Epoch: 0/1000  -- Batch:200/615\n",
      "     GenLoss 2.994  --  DiscLoss 5.735\n",
      "     D(x): 0.533  -- D(G(z)):0.532\n",
      "Epoch: 0/1000  -- Batch:300/615\n",
      "     GenLoss 3.049  --  DiscLoss 5.488\n",
      "     D(x): 0.504  -- D(G(z)):0.504\n",
      "Epoch: 0/1000  -- Batch:400/615\n",
      "     GenLoss 3.064  --  DiscLoss 5.664\n",
      "     D(x): 0.499  -- D(G(z)):0.499\n",
      "Epoch: 0/1000  -- Batch:500/615\n",
      "     GenLoss 3.095  --  DiscLoss 5.525\n",
      "     D(x): 0.499  -- D(G(z)):0.497\n",
      "Epoch: 0/1000  -- Batch:600/615\n",
      "     GenLoss 3.008  --  DiscLoss 5.508\n",
      "     D(x): 0.506  -- D(G(z)):0.498\n",
      "Testing on Users' datasset\n",
      "Testing on User 1 watermark:\n",
      "\n",
      "Test set: Average loss: 1.5134, Accuracy: 5857/5918 (99%)\n",
      "\n",
      "Testing on User 2 watermark:\n",
      "\n",
      "Test set: Average loss: 1.5561, Accuracy: 4830/6000 (80%)\n",
      "\n",
      "Testing on User 3 watermark:\n",
      "\n",
      "Test set: Average loss: 2.3874, Accuracy: 0/6742 (0%)\n",
      "\n",
      "Testing on clean test set\n",
      "\n",
      "Test set: Average loss: 2.4451, Accuracy: 957/10000 (10%)\n",
      "\n",
      "The use of a normal batch is 0.017028093338012695\n",
      "The time use of acgan is 0.16155219078063965\n",
      "Epoch: 1/1000  -- Batch:100/615\n",
      "     GenLoss 2.858  --  DiscLoss 5.291\n",
      "     D(x): 0.508  -- D(G(z)):0.494\n",
      "Epoch: 1/1000  -- Batch:200/615\n",
      "     GenLoss 2.718  --  DiscLoss 5.067\n",
      "     D(x): 0.513  -- D(G(z)):0.49\n",
      "Epoch: 1/1000  -- Batch:300/615\n",
      "     GenLoss 2.454  --  DiscLoss 4.889\n",
      "     D(x): 0.514  -- D(G(z)):0.487\n",
      "Epoch: 1/1000  -- Batch:400/615\n",
      "     GenLoss 2.432  --  DiscLoss 4.551\n",
      "     D(x): 0.517  -- D(G(z)):0.474\n",
      "Epoch: 1/1000  -- Batch:500/615\n",
      "     GenLoss 2.128  --  DiscLoss 4.157\n",
      "     D(x): 0.551  -- D(G(z)):0.471\n",
      "Epoch: 1/1000  -- Batch:600/615\n",
      "     GenLoss 2.158  --  DiscLoss 4.013\n",
      "     D(x): 0.545  -- D(G(z)):0.453\n",
      "Testing on Users' datasset\n",
      "Testing on User 1 watermark:\n",
      "\n",
      "Test set: Average loss: 1.1719, Accuracy: 5832/5918 (99%)\n",
      "\n",
      "Testing on User 2 watermark:\n",
      "\n",
      "Test set: Average loss: 0.6216, Accuracy: 5281/6000 (88%)\n",
      "\n",
      "Testing on User 3 watermark:\n",
      "\n",
      "Test set: Average loss: 2.1654, Accuracy: 0/6742 (0%)\n",
      "\n",
      "Testing on clean test set\n",
      "\n",
      "Test set: Average loss: 2.5195, Accuracy: 997/10000 (10%)\n",
      "\n",
      "The use of a normal batch is 0.01568436622619629\n",
      "The time use of acgan is 0.14844560623168945\n",
      "Epoch: 2/1000  -- Batch:100/615\n",
      "     GenLoss 2.071  --  DiscLoss 3.795\n",
      "     D(x): 0.556  -- D(G(z)):0.455\n",
      "Epoch: 2/1000  -- Batch:200/615\n",
      "     GenLoss 2.128  --  DiscLoss 3.898\n",
      "     D(x): 0.532  -- D(G(z)):0.459\n",
      "Epoch: 2/1000  -- Batch:300/615\n",
      "     GenLoss 2.076  --  DiscLoss 3.831\n",
      "     D(x): 0.525  -- D(G(z)):0.455\n",
      "Epoch: 2/1000  -- Batch:400/615\n",
      "     GenLoss 2.032  --  DiscLoss 4.032\n",
      "     D(x): 0.536  -- D(G(z)):0.453\n",
      "Epoch: 2/1000  -- Batch:500/615\n",
      "     GenLoss 1.859  --  DiscLoss 4.521\n",
      "     D(x): 0.483  -- D(G(z)):0.36\n",
      "Epoch: 2/1000  -- Batch:600/615\n",
      "     GenLoss 1.897  --  DiscLoss 4.331\n",
      "     D(x): 0.557  -- D(G(z)):0.54\n",
      "Testing on Users' datasset\n",
      "Testing on User 1 watermark:\n",
      "\n",
      "Test set: Average loss: 1.2865, Accuracy: 5754/5918 (97%)\n",
      "\n",
      "Testing on User 2 watermark:\n",
      "\n",
      "Test set: Average loss: 0.4416, Accuracy: 5806/6000 (97%)\n",
      "\n",
      "Testing on User 3 watermark:\n",
      "\n",
      "Test set: Average loss: 1.6468, Accuracy: 0/6742 (0%)\n",
      "\n",
      "Testing on clean test set\n",
      "\n",
      "Test set: Average loss: 2.2971, Accuracy: 2044/10000 (20%)\n",
      "\n",
      "The use of a normal batch is 0.014564037322998047\n",
      "The time use of acgan is 0.15971159934997559\n",
      "Epoch: 3/1000  -- Batch:100/615\n",
      "     GenLoss 1.793  --  DiscLoss 3.589\n",
      "     D(x): 0.503  -- D(G(z)):0.473\n",
      "Epoch: 3/1000  -- Batch:200/615\n",
      "     GenLoss 1.635  --  DiscLoss 3.641\n",
      "     D(x): 0.506  -- D(G(z)):0.47\n",
      "Epoch: 3/1000  -- Batch:300/615\n",
      "     GenLoss 1.518  --  DiscLoss 3.485\n",
      "     D(x): 0.5  -- D(G(z)):0.49\n",
      "Epoch: 3/1000  -- Batch:400/615\n",
      "     GenLoss 1.329  --  DiscLoss 3.253\n",
      "     D(x): 0.537  -- D(G(z)):0.523\n",
      "Epoch: 3/1000  -- Batch:500/615\n",
      "     GenLoss 1.456  --  DiscLoss 3.161\n",
      "     D(x): 0.523  -- D(G(z)):0.47\n",
      "Epoch: 3/1000  -- Batch:600/615\n",
      "     GenLoss 1.343  --  DiscLoss 3.426\n",
      "     D(x): 0.532  -- D(G(z)):0.514\n",
      "Testing on Users' datasset\n",
      "Testing on User 1 watermark:\n",
      "\n",
      "Test set: Average loss: 1.0670, Accuracy: 5823/5918 (98%)\n",
      "\n",
      "Testing on User 2 watermark:\n",
      "\n",
      "Test set: Average loss: 0.2143, Accuracy: 5959/6000 (99%)\n",
      "\n",
      "Testing on User 3 watermark:\n",
      "\n",
      "Test set: Average loss: 1.4556, Accuracy: 2862/6742 (42%)\n",
      "\n",
      "Testing on clean test set\n",
      "\n",
      "Test set: Average loss: 2.3450, Accuracy: 968/10000 (10%)\n",
      "\n",
      "The use of a normal batch is 0.012130498886108398\n",
      "The time use of acgan is 0.17493915557861328\n",
      "Epoch: 4/1000  -- Batch:100/615\n",
      "     GenLoss 1.31  --  DiscLoss 3.418\n",
      "     D(x): 0.499  -- D(G(z)):0.506\n",
      "Epoch: 4/1000  -- Batch:200/615\n",
      "     GenLoss 1.054  --  DiscLoss 2.821\n",
      "     D(x): 0.483  -- D(G(z)):0.488\n",
      "Epoch: 4/1000  -- Batch:300/615\n",
      "     GenLoss 0.925  --  DiscLoss 2.543\n",
      "     D(x): 0.495  -- D(G(z)):0.473\n",
      "Epoch: 4/1000  -- Batch:400/615\n",
      "     GenLoss 0.754  --  DiscLoss 2.411\n",
      "     D(x): 0.51  -- D(G(z)):0.505\n",
      "Epoch: 4/1000  -- Batch:500/615\n",
      "     GenLoss 0.742  --  DiscLoss 2.16\n",
      "     D(x): 0.504  -- D(G(z)):0.501\n",
      "Epoch: 4/1000  -- Batch:600/615\n",
      "     GenLoss 0.734  --  DiscLoss 2.043\n",
      "     D(x): 0.503  -- D(G(z)):0.484\n",
      "Testing on Users' datasset\n",
      "Testing on User 1 watermark:\n",
      "\n",
      "Test set: Average loss: 0.1008, Accuracy: 5917/5918 (100%)\n",
      "\n",
      "Testing on User 2 watermark:\n",
      "\n",
      "Test set: Average loss: 0.1562, Accuracy: 5667/6000 (94%)\n",
      "\n",
      "Testing on User 3 watermark:\n",
      "\n",
      "Test set: Average loss: 0.2491, Accuracy: 6592/6742 (98%)\n",
      "\n",
      "Testing on clean test set\n",
      "\n",
      "Test set: Average loss: 1.6546, Accuracy: 4106/10000 (41%)\n",
      "\n",
      "The use of a normal batch is 0.020493745803833008\n",
      "The time use of acgan is 0.14927244186401367\n",
      "Epoch: 5/1000  -- Batch:100/615\n",
      "     GenLoss 0.743  --  DiscLoss 2.187\n",
      "     D(x): 0.512  -- D(G(z)):0.503\n",
      "Epoch: 5/1000  -- Batch:200/615\n",
      "     GenLoss 0.71  --  DiscLoss 1.948\n",
      "     D(x): 0.516  -- D(G(z)):0.511\n",
      "Epoch: 5/1000  -- Batch:300/615\n",
      "     GenLoss 0.72  --  DiscLoss 1.849\n",
      "     D(x): 0.507  -- D(G(z)):0.498\n",
      "Epoch: 5/1000  -- Batch:400/615\n",
      "     GenLoss 0.759  --  DiscLoss 1.937\n",
      "     D(x): 0.547  -- D(G(z)):0.482\n",
      "Epoch: 5/1000  -- Batch:500/615\n",
      "     GenLoss 1.092  --  DiscLoss 2.862\n",
      "     D(x): 0.47  -- D(G(z)):0.462\n",
      "Epoch: 5/1000  -- Batch:600/615\n",
      "     GenLoss 0.746  --  DiscLoss 1.968\n",
      "     D(x): 0.486  -- D(G(z)):0.485\n",
      "Testing on Users' datasset\n",
      "Testing on User 1 watermark:\n",
      "\n",
      "Test set: Average loss: 0.1356, Accuracy: 5895/5918 (100%)\n",
      "\n",
      "Testing on User 2 watermark:\n",
      "\n",
      "Test set: Average loss: 0.2306, Accuracy: 5662/6000 (94%)\n",
      "\n",
      "Testing on User 3 watermark:\n",
      "\n",
      "Test set: Average loss: 0.3310, Accuracy: 6654/6742 (99%)\n",
      "\n",
      "Testing on clean test set\n",
      "\n",
      "Test set: Average loss: 1.4048, Accuracy: 4881/10000 (49%)\n",
      "\n",
      "The use of a normal batch is 0.019207239151000977\n",
      "The time use of acgan is 0.16323232650756836\n",
      "Epoch: 6/1000  -- Batch:100/615\n",
      "     GenLoss 0.655  --  DiscLoss 1.958\n",
      "     D(x): 0.534  -- D(G(z)):0.534\n",
      "Epoch: 6/1000  -- Batch:200/615\n",
      "     GenLoss 0.726  --  DiscLoss 1.859\n",
      "     D(x): 0.475  -- D(G(z)):0.486\n",
      "Epoch: 6/1000  -- Batch:300/615\n",
      "     GenLoss 0.778  --  DiscLoss 1.92\n",
      "     D(x): 0.455  -- D(G(z)):0.47\n",
      "Epoch: 6/1000  -- Batch:400/615\n",
      "     GenLoss 0.585  --  DiscLoss 1.687\n",
      "     D(x): 0.52  -- D(G(z)):0.54\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 75\u001b[0m\n\u001b[1;32m     73\u001b[0m     optimizerD\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     74\u001b[0m     optimizerG\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> 75\u001b[0m     lossG\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     76\u001b[0m     optimizerG\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     77\u001b[0m \u001b[39mif\u001b[39;00m batch_idx \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m/home/data/anaconda3/envs/pytorch_1.13/lib/python3.8/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m/home/data/anaconda3/envs/pytorch_1.13/lib/python3.8/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    199\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "total_step = len(train_loader)\n",
    "time1 = time.time()\n",
    "time_list = []\n",
    "User1 = []\n",
    "User2 = []\n",
    "User3 = []\n",
    "Clean = []\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    for batch_idx, data in enumerate(zip(train_loader, cycle(adv_loader))):\n",
    "        time2 = time.time()\n",
    "        x, target = data[0]\n",
    "        images = x.to(device)\n",
    "        target = torch.LongTensor(target).to(device)\n",
    "        # TRAIN D\n",
    "        # On true data in collaborative learning\n",
    "        froze_layer(D.l_gan_logit)\n",
    "        predictR, predictRLabel = D(images) #image from the real dataset\n",
    "        loss_real_aux = criterion_aux(predictRLabel, target)\n",
    "        optimizerD.zero_grad()\n",
    "        optimizerG.zero_grad()\n",
    "        loss_real_aux.backward()\n",
    "        optimizerD.step()\n",
    "\n",
    "        if batch_idx ==0:\n",
    "            time_normal = time.time()-time2\n",
    "            print('The use of a normal batch is {}'.format(time_normal))\n",
    "        real_score = predictR\n",
    "\n",
    "        activate_layer(D.l_gan_logit)\n",
    "\n",
    "        # On MI data\n",
    "        time3 = time.time()\n",
    "        x, target = data[1]\n",
    "        images = x.to(device)\n",
    "        target = torch.LongTensor(target).to(device)\n",
    "\n",
    "        current_batchSize = images.size()[0]\n",
    "        realLabel = torch.ones(current_batchSize).to(device)\n",
    "        fakeLabel = torch.zeros(current_batchSize).to(device)\n",
    "\n",
    "        predictR, predictRLabel = D(images)\n",
    "        loss_real_aux = criterion_aux(predictRLabel, target)\n",
    "        loss_real_adv = criterion_adv(predictR, realLabel)\n",
    "        real_score = predictR\n",
    "\n",
    "        # On fake data\n",
    "        #latent_value = torch.randn(current_batchSize, 128).to(device)\n",
    "        latent_value = torch.normal(0,10,(current_batchSize, 128)).to(device)\n",
    "        gen_labels = torch.LongTensor(np.random.randint(0, nb_classes, current_batchSize)).to(device)\n",
    "        fake_images = G(latent_value , gen_labels) #generate a fake image\n",
    "        predictF, predictFLabel = D(fake_images)\n",
    "        loss_fake_adv = criterion_adv(predictF ,  fakeLabel) # compare vs label =0 (D is supposed to \"understand\" that the image generated by G is fake)\n",
    "        loss_fake_aux = criterion_aux(predictFLabel, gen_labels)\n",
    "        fake_score = predictF\n",
    "\n",
    "        lossD = loss_real_adv + loss_real_aux  +loss_fake_adv + loss_fake_aux\n",
    "\n",
    "        optimizerD.zero_grad()\n",
    "        optimizerG.zero_grad()\n",
    "        lossD.backward()\n",
    "        optimizerD.step()\n",
    "        for i in range(6): \n",
    "        # TRAIN G\n",
    "            #latent_value = torch.randn(current_batchSize, 128).to(device)\n",
    "            latent_value = torch.normal(0,10,(current_batchSize, 128)).to(device)\n",
    "            gen_labels = torch.LongTensor(np.random.randint(0, nb_classes, current_batchSize)).to(device)\n",
    "            fake_images= G(latent_value, gen_labels) #Generate a fake image\n",
    "            predictG, predictLabel = D(fake_images)\n",
    "            lossG_adv = criterion_adv(predictG, realLabel) # Compare vs label = 1 (We want to optimize G to fool D, predictG must tend to 1)\n",
    "            lossG_aux = criterion_aux(predictLabel, gen_labels)\n",
    "            lossG = lossG_adv + lossG_aux\n",
    "            optimizerD.zero_grad()\n",
    "            optimizerG.zero_grad()\n",
    "            lossG.backward()\n",
    "            optimizerG.step()\n",
    "        if batch_idx == 0:\n",
    "            time_acgan = time.time()-time3\n",
    "            print('The time use of acgan is {}'.format(time_acgan))\n",
    "\n",
    "        if (batch_idx+1) % 100 == 0:\n",
    "            print(\"Epoch: \"+str(epoch)+\"/\"+str(num_epochs)+ \"  -- Batch:\"+ str(batch_idx+1)+\"/\"+str(total_step))\n",
    "            print(\"     GenLoss \"+str(round(lossG.item(), 3))+ \"  --  DiscLoss \"+str(round(lossD.item(), 3)))\n",
    "            print(\"     D(x): \"+str(round(real_score.mean().item(), 3))+ \"  -- D(G(z)):\"+str(round(fake_score.mean().item(), 3)))\n",
    "\n",
    "    #scheduler.step()\n",
    "    time_list.append(round(time.time()-time1,2))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        fake_images = fake_images.reshape(fake_images.size(0), 3, 32, 32)\n",
    "        save_image(denorm(fake_images), os.path.join(resul_dir, 'fake_images-{}.png'.format(epoch+1)))\n",
    "    if (epoch+1) == 1:\n",
    "        save_image(images, os.path.join(resul_dir, 'real_images.png'),  normalize = True)\n",
    "    \n",
    "        \n",
    "    test = comprehensive_user_test(D, device, test_loader, user_wm_dataset)\n",
    "    #User1.append(round(test[0],2))\n",
    "    #User2.append(round(test[1],2))\n",
    "    #User3.append(round(test[2],2))\n",
    "    #Clean.append(round(test[3],2))\n",
    "    #print(time_list)\n",
    "    #print(User1)\n",
    "    #print(User2)\n",
    "    #print(User3)\n",
    "    #print(Clean)\n",
    "    \n",
    "    torch.save(G.state_dict(), 'G-CL.pth')\n",
    "    torch.save(D.state_dict(), 'D-CL.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m运行具有“pytorch_1.13”的单元格需要ipykernel包。\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n pytorch_1.13 ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "nbImageToGenerate = 8*8\n",
    "for i in range(10):\n",
    "    latent_value = torch.randn((nbImageToGenerate, 128)).to(device)\n",
    "    gen_labels = torch.LongTensor(np.full(nbImageToGenerate , i )).to(device)\n",
    "    fake_images = G(latent_value , gen_labels) #Generate a fake image\n",
    "    fake_images = fake_images.reshape(fake_images.size(0), 3, 32, 32)\n",
    "    save_image(denorm(fake_images), os.path.join(resul_dir, 'GeneratedSample-{}.png'.format(i)))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9022a0ab266fa41286615ab4189789096e81cf7e07d840c9ac23f04d2f63b2aa"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
