{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from itertools import cycle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from numpy.random import randint\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "from model_init import *\n",
    "from dataset_init import *\n",
    "from utils.others import *\n",
    "from utils.testModel import *\n",
    "import time\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Device  cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Current Device \" , device)\n",
    "\n",
    "resul_dir = './results'\n",
    "if not os.path.exists(resul_dir):\n",
    "    os.makedirs(resul_dir)\n",
    "\n",
    "batch_size = 128\n",
    "num_epochs = 1000\n",
    "num_users = 6\n",
    "nb_classes = 10\n",
    "dataset = 'MNIST'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n"
     ]
    }
   ],
   "source": [
    "clean_dataset, train_dataset, user_wm_dataset, test_dataset = dataset_init(dataset, num_users=num_users)\n",
    "\n",
    "adv_mi_dataset = Subset(clean_dataset, randint(0, len(clean_dataset),size=300)) # 1800 300\n",
    "\n",
    "for i in range(num_users-1):\n",
    "    adv_mi_dataset += Subset(user_wm_dataset[i], randint(0, len(user_wm_dataset[i]),size=300))\n",
    "print(len(clean_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, num_workers=4, shuffle=True)\n",
    "\n",
    "adv_loader = DataLoader(adv_mi_dataset, batch_size=batch_size, num_workers=4, shuffle=True, drop_last=True)\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=1000, num_workers=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "D, G = model_init(dataset, device)\n",
    "#optimizerD = torch.optim.Adam(filter(lambda p: p.requires_grad, D.parameters()), 0.0002, betas=(0.5, 0.999))\n",
    "optimizerG = torch.optim.Adam(G.parameters(), 0.0002, betas=(0.5, 0.999))\n",
    "\n",
    "optimizerD = torch.optim.SGD(filter(lambda p: p.requires_grad, D.parameters()),lr=0.01, momentum=0.9, weight_decay=0.0001)\n",
    "# scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizerD, [10, 20], gamma=0.1)\n",
    "criterion_adv = nn.BCELoss()\n",
    "criterion_aux = nn.CrossEntropyLoss() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The use of a normal batch is 0.14819598197937012\n",
      "The time use of acgan is 0.17349743843078613\n",
      "Epoch: 0/1000  -- Batch:100/516\n",
      "     GenLoss 3.076  --  DiscLoss 5.815\n",
      "     D(x): 0.5  -- D(G(z)):0.5\n",
      "Epoch: 0/1000  -- Batch:200/516\n",
      "     GenLoss 1.953  --  DiscLoss 4.584\n",
      "     D(x): 0.445  -- D(G(z)):0.435\n",
      "Epoch: 0/1000  -- Batch:300/516\n",
      "     GenLoss 0.674  --  DiscLoss 2.51\n",
      "     D(x): 0.502  -- D(G(z)):0.528\n",
      "Epoch: 0/1000  -- Batch:400/516\n",
      "     GenLoss 0.647  --  DiscLoss 2.539\n",
      "     D(x): 0.557  -- D(G(z)):0.586\n",
      "Epoch: 0/1000  -- Batch:500/516\n",
      "     GenLoss 0.507  --  DiscLoss 2.156\n",
      "     D(x): 0.53  -- D(G(z)):0.603\n",
      "Testing on Users' datasset\n",
      "Testing on User 0 watermark:\n",
      "\n",
      "Test set: Average loss: 0.0273, Accuracy: 967/967 (100%)\n",
      "\n",
      "Testing on User 1 watermark:\n",
      "\n",
      "Test set: Average loss: 0.4586, Accuracy: 1126/1138 (99%)\n",
      "\n",
      "Testing on User 2 watermark:\n",
      "\n",
      "Test set: Average loss: 0.6376, Accuracy: 890/1003 (89%)\n",
      "\n",
      "Testing on User 3 watermark:\n",
      "\n",
      "Test set: Average loss: 0.1699, Accuracy: 1026/1027 (100%)\n",
      "\n",
      "Testing on User 4 watermark:\n",
      "\n",
      "Test set: Average loss: 0.1258, Accuracy: 959/1000 (96%)\n",
      "\n",
      "Testing on User 5 watermark:\n",
      "\n",
      "Test set: Average loss: 2.7897, Accuracy: 30/873 (3%)\n",
      "\n",
      "Testing on clean test set\n",
      "\n",
      "Test set: Average loss: 1.2058, Accuracy: 5254/10000 (53%)\n",
      "\n",
      "The use of a normal batch is 0.017035245895385742\n",
      "The time use of acgan is 0.15412664413452148\n",
      "Epoch: 1/1000  -- Batch:100/516\n",
      "     GenLoss 1.369  --  DiscLoss 3.292\n",
      "     D(x): 0.409  -- D(G(z)):0.357\n",
      "Epoch: 1/1000  -- Batch:200/516\n",
      "     GenLoss 0.687  --  DiscLoss 1.682\n",
      "     D(x): 0.536  -- D(G(z)):0.548\n",
      "Epoch: 1/1000  -- Batch:300/516\n",
      "     GenLoss 0.568  --  DiscLoss 1.714\n",
      "     D(x): 0.557  -- D(G(z)):0.617\n",
      "Epoch: 1/1000  -- Batch:400/516\n",
      "     GenLoss 0.679  --  DiscLoss 2.195\n",
      "     D(x): 0.483  -- D(G(z)):0.512\n",
      "Epoch: 1/1000  -- Batch:500/516\n",
      "     GenLoss 0.666  --  DiscLoss 1.601\n",
      "     D(x): 0.513  -- D(G(z)):0.526\n",
      "Testing on Users' datasset\n",
      "Testing on User 0 watermark:\n",
      "\n",
      "Test set: Average loss: 0.0220, Accuracy: 967/967 (100%)\n",
      "\n",
      "Testing on User 1 watermark:\n",
      "\n",
      "Test set: Average loss: 0.0567, Accuracy: 1135/1138 (100%)\n",
      "\n",
      "Testing on User 2 watermark:\n",
      "\n",
      "Test set: Average loss: 0.5654, Accuracy: 803/1003 (80%)\n",
      "\n",
      "Testing on User 3 watermark:\n",
      "\n",
      "Test set: Average loss: 0.0346, Accuracy: 1023/1027 (100%)\n",
      "\n",
      "Testing on User 4 watermark:\n",
      "\n",
      "Test set: Average loss: 0.1443, Accuracy: 978/1000 (98%)\n",
      "\n",
      "Testing on User 5 watermark:\n",
      "\n",
      "Test set: Average loss: 3.3677, Accuracy: 30/873 (3%)\n",
      "\n",
      "Testing on clean test set\n",
      "\n",
      "Test set: Average loss: 0.3123, Accuracy: 9087/10000 (91%)\n",
      "\n",
      "The use of a normal batch is 0.01732778549194336\n",
      "The time use of acgan is 0.15368986129760742\n",
      "Epoch: 2/1000  -- Batch:100/516\n",
      "     GenLoss 0.677  --  DiscLoss 1.532\n",
      "     D(x): 0.467  -- D(G(z)):0.491\n",
      "Epoch: 2/1000  -- Batch:200/516\n",
      "     GenLoss 0.679  --  DiscLoss 1.585\n",
      "     D(x): 0.44  -- D(G(z)):0.502\n",
      "Epoch: 2/1000  -- Batch:300/516\n",
      "     GenLoss 0.567  --  DiscLoss 1.512\n",
      "     D(x): 0.536  -- D(G(z)):0.566\n",
      "Epoch: 2/1000  -- Batch:400/516\n",
      "     GenLoss 0.687  --  DiscLoss 1.509\n",
      "     D(x): 0.469  -- D(G(z)):0.507\n",
      "Epoch: 2/1000  -- Batch:500/516\n",
      "     GenLoss 0.718  --  DiscLoss 1.47\n",
      "     D(x): 0.5  -- D(G(z)):0.514\n",
      "Testing on Users' datasset\n",
      "Testing on User 0 watermark:\n",
      "\n",
      "Test set: Average loss: 0.0144, Accuracy: 964/967 (100%)\n",
      "\n",
      "Testing on User 1 watermark:\n",
      "\n",
      "Test set: Average loss: 0.0247, Accuracy: 1134/1138 (100%)\n",
      "\n",
      "Testing on User 2 watermark:\n",
      "\n",
      "Test set: Average loss: 0.0399, Accuracy: 999/1003 (100%)\n",
      "\n",
      "Testing on User 3 watermark:\n",
      "\n",
      "Test set: Average loss: 0.0027, Accuracy: 1027/1027 (100%)\n",
      "\n",
      "Testing on User 4 watermark:\n",
      "\n",
      "Test set: Average loss: 0.0139, Accuracy: 995/1000 (100%)\n",
      "\n",
      "Testing on User 5 watermark:\n",
      "\n",
      "Test set: Average loss: 2.4956, Accuracy: 29/873 (3%)\n",
      "\n",
      "Testing on clean test set\n",
      "\n",
      "Test set: Average loss: 0.1806, Accuracy: 9421/10000 (94%)\n",
      "\n",
      "The use of a normal batch is 0.015058755874633789\n",
      "The time use of acgan is 0.1577165126800537\n",
      "Epoch: 3/1000  -- Batch:100/516\n",
      "     GenLoss 0.742  --  DiscLoss 1.46\n",
      "     D(x): 0.474  -- D(G(z)):0.483\n",
      "Epoch: 3/1000  -- Batch:200/516\n",
      "     GenLoss 0.7  --  DiscLoss 1.436\n",
      "     D(x): 0.484  -- D(G(z)):0.502\n",
      "Epoch: 3/1000  -- Batch:300/516\n",
      "     GenLoss 0.676  --  DiscLoss 1.427\n",
      "     D(x): 0.503  -- D(G(z)):0.506\n",
      "Epoch: 3/1000  -- Batch:400/516\n",
      "     GenLoss 0.695  --  DiscLoss 1.412\n",
      "     D(x): 0.501  -- D(G(z)):0.508\n",
      "Epoch: 3/1000  -- Batch:500/516\n",
      "     GenLoss 0.667  --  DiscLoss 1.527\n",
      "     D(x): 0.502  -- D(G(z)):0.514\n",
      "Testing on Users' datasset\n",
      "Testing on User 0 watermark:\n",
      "\n",
      "Test set: Average loss: 0.0178, Accuracy: 963/967 (100%)\n",
      "\n",
      "Testing on User 1 watermark:\n",
      "\n",
      "Test set: Average loss: 0.0017, Accuracy: 1137/1138 (100%)\n",
      "\n",
      "Testing on User 2 watermark:\n",
      "\n",
      "Test set: Average loss: 0.0414, Accuracy: 992/1003 (99%)\n",
      "\n",
      "Testing on User 3 watermark:\n",
      "\n",
      "Test set: Average loss: 0.0001, Accuracy: 1027/1027 (100%)\n",
      "\n",
      "Testing on User 4 watermark:\n",
      "\n",
      "Test set: Average loss: 0.0084, Accuracy: 998/1000 (100%)\n",
      "\n",
      "Testing on User 5 watermark:\n",
      "\n",
      "Test set: Average loss: 5.0269, Accuracy: 2/873 (0%)\n",
      "\n",
      "Testing on clean test set\n",
      "\n",
      "Test set: Average loss: 0.1096, Accuracy: 9672/10000 (97%)\n",
      "\n",
      "The use of a normal batch is 0.019896984100341797\n",
      "The time use of acgan is 0.16121292114257812\n",
      "Epoch: 4/1000  -- Batch:100/516\n",
      "     GenLoss 0.701  --  DiscLoss 1.447\n",
      "     D(x): 0.489  -- D(G(z)):0.508\n",
      "Epoch: 4/1000  -- Batch:200/516\n",
      "     GenLoss 0.654  --  DiscLoss 1.412\n",
      "     D(x): 0.508  -- D(G(z)):0.514\n",
      "Epoch: 4/1000  -- Batch:300/516\n",
      "     GenLoss 0.693  --  DiscLoss 1.413\n",
      "     D(x): 0.501  -- D(G(z)):0.51\n",
      "Epoch: 4/1000  -- Batch:400/516\n",
      "     GenLoss 0.654  --  DiscLoss 1.441\n",
      "     D(x): 0.515  -- D(G(z)):0.519\n",
      "Epoch: 4/1000  -- Batch:500/516\n",
      "     GenLoss 0.632  --  DiscLoss 1.479\n",
      "     D(x): 0.502  -- D(G(z)):0.534\n",
      "Testing on Users' datasset\n",
      "Testing on User 0 watermark:\n",
      "\n",
      "Test set: Average loss: 0.0045, Accuracy: 966/967 (100%)\n",
      "\n",
      "Testing on User 1 watermark:\n",
      "\n",
      "Test set: Average loss: 0.0013, Accuracy: 1138/1138 (100%)\n",
      "\n",
      "Testing on User 2 watermark:\n",
      "\n",
      "Test set: Average loss: 0.0179, Accuracy: 999/1003 (100%)\n",
      "\n",
      "Testing on User 3 watermark:\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 1027/1027 (100%)\n",
      "\n",
      "Testing on User 4 watermark:\n",
      "\n",
      "Test set: Average loss: 0.0017, Accuracy: 999/1000 (100%)\n",
      "\n",
      "Testing on User 5 watermark:\n",
      "\n",
      "Test set: Average loss: 1.9288, Accuracy: 20/873 (2%)\n",
      "\n",
      "Testing on clean test set\n",
      "\n",
      "Test set: Average loss: 0.0675, Accuracy: 9784/10000 (98%)\n",
      "\n",
      "The use of a normal batch is 0.021799325942993164\n",
      "The time use of acgan is 0.16212725639343262\n",
      "Epoch: 5/1000  -- Batch:100/516\n",
      "     GenLoss 0.68  --  DiscLoss 1.41\n",
      "     D(x): 0.49  -- D(G(z)):0.497\n",
      "Epoch: 5/1000  -- Batch:200/516\n",
      "     GenLoss 0.708  --  DiscLoss 1.406\n",
      "     D(x): 0.499  -- D(G(z)):0.503\n",
      "Epoch: 5/1000  -- Batch:300/516\n",
      "     GenLoss 0.724  --  DiscLoss 1.404\n",
      "     D(x): 0.476  -- D(G(z)):0.479\n",
      "Epoch: 5/1000  -- Batch:400/516\n",
      "     GenLoss 0.752  --  DiscLoss 1.392\n",
      "     D(x): 0.478  -- D(G(z)):0.474\n",
      "Epoch: 5/1000  -- Batch:500/516\n",
      "     GenLoss 0.688  --  DiscLoss 1.404\n",
      "     D(x): 0.497  -- D(G(z)):0.501\n",
      "Testing on Users' datasset\n",
      "Testing on User 0 watermark:\n",
      "\n",
      "Test set: Average loss: 0.0005, Accuracy: 967/967 (100%)\n",
      "\n",
      "Testing on User 1 watermark:\n",
      "\n",
      "Test set: Average loss: 0.0003, Accuracy: 1138/1138 (100%)\n",
      "\n",
      "Testing on User 2 watermark:\n",
      "\n",
      "Test set: Average loss: 0.0338, Accuracy: 993/1003 (99%)\n",
      "\n",
      "Testing on User 3 watermark:\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 1027/1027 (100%)\n",
      "\n",
      "Testing on User 4 watermark:\n",
      "\n",
      "Test set: Average loss: 0.0085, Accuracy: 999/1000 (100%)\n",
      "\n",
      "Testing on User 5 watermark:\n",
      "\n",
      "Test set: Average loss: 1.2472, Accuracy: 194/873 (22%)\n",
      "\n",
      "Testing on clean test set\n",
      "\n",
      "Test set: Average loss: 0.0620, Accuracy: 9790/10000 (98%)\n",
      "\n",
      "The use of a normal batch is 0.024496793746948242\n",
      "The time use of acgan is 0.16235613822937012\n",
      "Epoch: 6/1000  -- Batch:100/516\n",
      "     GenLoss 0.664  --  DiscLoss 1.421\n",
      "     D(x): 0.506  -- D(G(z)):0.517\n",
      "Epoch: 6/1000  -- Batch:200/516\n",
      "     GenLoss 0.684  --  DiscLoss 1.39\n",
      "     D(x): 0.511  -- D(G(z)):0.504\n",
      "Epoch: 6/1000  -- Batch:300/516\n",
      "     GenLoss 0.724  --  DiscLoss 1.417\n",
      "     D(x): 0.493  -- D(G(z)):0.494\n",
      "Epoch: 6/1000  -- Batch:400/516\n",
      "     GenLoss 0.676  --  DiscLoss 1.413\n",
      "     D(x): 0.494  -- D(G(z)):0.503\n",
      "Epoch: 6/1000  -- Batch:500/516\n",
      "     GenLoss 0.744  --  DiscLoss 1.407\n",
      "     D(x): 0.468  -- D(G(z)):0.469\n",
      "Testing on Users' datasset\n",
      "Testing on User 0 watermark:\n",
      "\n",
      "Test set: Average loss: 0.0003, Accuracy: 967/967 (100%)\n",
      "\n",
      "Testing on User 1 watermark:\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 1138/1138 (100%)\n",
      "\n",
      "Testing on User 2 watermark:\n",
      "\n",
      "Test set: Average loss: 0.0577, Accuracy: 988/1003 (99%)\n",
      "\n",
      "Testing on User 3 watermark:\n",
      "\n",
      "Test set: Average loss: 0.0003, Accuracy: 1027/1027 (100%)\n",
      "\n",
      "Testing on User 4 watermark:\n",
      "\n",
      "Test set: Average loss: 0.0091, Accuracy: 996/1000 (100%)\n",
      "\n",
      "Testing on User 5 watermark:\n",
      "\n",
      "Test set: Average loss: 7.1121, Accuracy: 0/873 (0%)\n",
      "\n",
      "Testing on clean test set\n",
      "\n",
      "Test set: Average loss: 0.0658, Accuracy: 9806/10000 (98%)\n",
      "\n",
      "The use of a normal batch is 0.017360448837280273\n",
      "The time use of acgan is 0.15180134773254395\n",
      "Epoch: 7/1000  -- Batch:100/516\n",
      "     GenLoss 0.718  --  DiscLoss 1.399\n",
      "     D(x): 0.495  -- D(G(z)):0.494\n",
      "Epoch: 7/1000  -- Batch:200/516\n",
      "     GenLoss 0.711  --  DiscLoss 1.42\n",
      "     D(x): 0.48  -- D(G(z)):0.489\n",
      "Epoch: 7/1000  -- Batch:300/516\n",
      "     GenLoss 0.717  --  DiscLoss 1.429\n",
      "     D(x): 0.494  -- D(G(z)):0.502\n",
      "Epoch: 7/1000  -- Batch:400/516\n",
      "     GenLoss 0.679  --  DiscLoss 1.387\n",
      "     D(x): 0.501  -- D(G(z)):0.499\n",
      "Epoch: 7/1000  -- Batch:500/516\n",
      "     GenLoss 0.707  --  DiscLoss 1.411\n",
      "     D(x): 0.484  -- D(G(z)):0.489\n",
      "Testing on Users' datasset\n",
      "Testing on User 0 watermark:\n",
      "\n",
      "Test set: Average loss: 0.0001, Accuracy: 967/967 (100%)\n",
      "\n",
      "Testing on User 1 watermark:\n",
      "\n",
      "Test set: Average loss: 0.0001, Accuracy: 1138/1138 (100%)\n",
      "\n",
      "Testing on User 2 watermark:\n",
      "\n",
      "Test set: Average loss: 0.0006, Accuracy: 1003/1003 (100%)\n",
      "\n",
      "Testing on User 3 watermark:\n",
      "\n",
      "Test set: Average loss: 0.0002, Accuracy: 1027/1027 (100%)\n",
      "\n",
      "Testing on User 4 watermark:\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 1000/1000 (100%)\n",
      "\n",
      "Testing on User 5 watermark:\n",
      "\n",
      "Test set: Average loss: 0.0105, Accuracy: 873/873 (100%)\n",
      "\n",
      "Testing on clean test set\n",
      "\n",
      "Test set: Average loss: 0.0689, Accuracy: 9770/10000 (98%)\n",
      "\n",
      "The use of a normal batch is 0.019348621368408203\n",
      "The time use of acgan is 0.15787148475646973\n",
      "Epoch: 8/1000  -- Batch:100/516\n",
      "     GenLoss 0.701  --  DiscLoss 1.388\n",
      "     D(x): 0.491  -- D(G(z)):0.488\n",
      "Epoch: 8/1000  -- Batch:200/516\n",
      "     GenLoss 0.663  --  DiscLoss 1.416\n",
      "     D(x): 0.498  -- D(G(z)):0.508\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/zcy/MODA/train_ACGAN.ipynb Cell 6\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bregion-8.seetacloud.com/home/zcy/MODA/train_ACGAN.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=58'>59</a>\u001b[0m optimizerD\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bregion-8.seetacloud.com/home/zcy/MODA/train_ACGAN.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=59'>60</a>\u001b[0m optimizerG\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bregion-8.seetacloud.com/home/zcy/MODA/train_ACGAN.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=60'>61</a>\u001b[0m lossD\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bregion-8.seetacloud.com/home/zcy/MODA/train_ACGAN.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=61'>62</a>\u001b[0m optimizerD\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bregion-8.seetacloud.com/home/zcy/MODA/train_ACGAN.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=62'>63</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m6\u001b[39m): \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bregion-8.seetacloud.com/home/zcy/MODA/train_ACGAN.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=63'>64</a>\u001b[0m \u001b[39m# TRAIN G\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bregion-8.seetacloud.com/home/zcy/MODA/train_ACGAN.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=64'>65</a>\u001b[0m     \u001b[39m#latent_value = torch.randn(current_batchSize, 128).to(device)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/_tensor.py:363\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    355\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    356\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    357\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    361\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[1;32m    362\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[0;32m--> 363\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    175\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "total_step = len(train_loader)\n",
    "time1 = time.time()\n",
    "time_list = []\n",
    "User1 = []\n",
    "User2 = []\n",
    "User3 = []\n",
    "Clean = []\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    for batch_idx, data in enumerate(zip(train_loader, cycle(adv_loader))):\n",
    "        time2 = time.time()\n",
    "        x, target = data[0]\n",
    "        images = x.to(device)\n",
    "        target = torch.LongTensor(target).to(device)\n",
    "        # TRAIN D\n",
    "        # On true data in collaborative learning\n",
    "        froze_layer(D.l_gan_logit)\n",
    "        predictR, predictRLabel = D(images) #image from the real dataset\n",
    "        loss_real_aux = criterion_aux(predictRLabel, target)\n",
    "        optimizerD.zero_grad()\n",
    "        optimizerG.zero_grad()\n",
    "        loss_real_aux.backward()\n",
    "        optimizerD.step()\n",
    "\n",
    "        if batch_idx ==0:\n",
    "            time_normal = time.time()-time2\n",
    "            print('The use of a normal batch is {}'.format(time_normal))\n",
    "        real_score = predictR\n",
    "\n",
    "        activate_layer(D.l_gan_logit)\n",
    "\n",
    "        # On MI data\n",
    "        time3 = time.time()\n",
    "        x, target = data[1]\n",
    "        images = x.to(device)\n",
    "        target = torch.LongTensor(target).to(device)\n",
    "\n",
    "        current_batchSize = images.size()[0]\n",
    "        realLabel = torch.ones(current_batchSize).to(device)\n",
    "        fakeLabel = torch.zeros(current_batchSize).to(device)\n",
    "\n",
    "        predictR, predictRLabel = D(images)\n",
    "        loss_real_aux = criterion_aux(predictRLabel, target)\n",
    "        loss_real_adv = criterion_adv(predictR, realLabel)\n",
    "        real_score = predictR\n",
    "\n",
    "        # On fake data\n",
    "        #latent_value = torch.randn(current_batchSize, 128).to(device)\n",
    "        latent_value = torch.normal(0,10,(current_batchSize, 128)).to(device)\n",
    "        gen_labels = torch.LongTensor(np.random.randint(0, nb_classes, current_batchSize)).to(device)\n",
    "        fake_images = G(latent_value , gen_labels) #generate a fake image\n",
    "        predictF, predictFLabel = D(fake_images)\n",
    "        loss_fake_adv = criterion_adv(predictF ,  fakeLabel) # compare vs label =0 (D is supposed to \"understand\" that the image generated by G is fake)\n",
    "        loss_fake_aux = criterion_aux(predictFLabel, gen_labels)\n",
    "        fake_score = predictF\n",
    "\n",
    "        lossD = loss_real_adv + loss_real_aux  +loss_fake_adv + loss_fake_aux\n",
    "\n",
    "        optimizerD.zero_grad()\n",
    "        optimizerG.zero_grad()\n",
    "        lossD.backward()\n",
    "        optimizerD.step()\n",
    "        for i in range(6): \n",
    "        # TRAIN G\n",
    "            #latent_value = torch.randn(current_batchSize, 128).to(device)\n",
    "            latent_value = torch.normal(0,10,(current_batchSize, 128)).to(device)\n",
    "            gen_labels = torch.LongTensor(np.random.randint(0, nb_classes, current_batchSize)).to(device)\n",
    "            fake_images= G(latent_value, gen_labels) #Generate a fake image\n",
    "            predictG, predictLabel = D(fake_images)\n",
    "            lossG_adv = criterion_adv(predictG, realLabel) # Compare vs label = 1 (We want to optimize G to fool D, predictG must tend to 1)\n",
    "            lossG_aux = criterion_aux(predictLabel, gen_labels)\n",
    "            lossG = lossG_adv + lossG_aux\n",
    "            optimizerD.zero_grad()\n",
    "            optimizerG.zero_grad()\n",
    "            lossG.backward()\n",
    "            optimizerG.step()\n",
    "        if batch_idx == 0:\n",
    "            time_acgan = time.time()-time3\n",
    "            print('The time use of acgan is {}'.format(time_acgan))\n",
    "\n",
    "        if (batch_idx+1) % 100 == 0:\n",
    "            print(\"Epoch: \"+str(epoch)+\"/\"+str(num_epochs)+ \"  -- Batch:\"+ str(batch_idx+1)+\"/\"+str(total_step))\n",
    "            print(\"     GenLoss \"+str(round(lossG.item(), 3))+ \"  --  DiscLoss \"+str(round(lossD.item(), 3)))\n",
    "            print(\"     D(x): \"+str(round(real_score.mean().item(), 3))+ \"  -- D(G(z)):\"+str(round(fake_score.mean().item(), 3)))\n",
    "\n",
    "    #scheduler.step()\n",
    "    time_list.append(round(time.time()-time1,2))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        fake_images = fake_images.reshape(fake_images.size(0), 3, 32, 32)\n",
    "        save_image(denorm(fake_images), os.path.join(resul_dir, 'fake_images-{}.png'.format(epoch+1)))\n",
    "    if (epoch+1) == 1:\n",
    "        save_image(images, os.path.join(resul_dir, 'real_images.png'),  normalize = True)\n",
    "    \n",
    "        \n",
    "    test = comprehensive_user_test(D, device, test_loader, user_wm_dataset)\n",
    "    #User1.append(round(test[0],2))\n",
    "    #User2.append(round(test[1],2))\n",
    "    #User3.append(round(test[2],2))\n",
    "    #Clean.append(round(test[3],2))\n",
    "    #print(time_list)\n",
    "    #print(User1)\n",
    "    #print(User2)\n",
    "    #print(User3)\n",
    "    #print(Clean)\n",
    "    \n",
    "    torch.save(G.state_dict(), 'G-CL.pth')\n",
    "    torch.save(D.state_dict(), 'D-CL.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbImageToGenerate = 8*8\n",
    "for i in range(10):\n",
    "    latent_value = torch.randn((nbImageToGenerate, 128)).to(device)\n",
    "    gen_labels = torch.LongTensor(np.full(nbImageToGenerate , i )).to(device)\n",
    "    fake_images = G(latent_value , gen_labels) #Generate a fake image\n",
    "    fake_images = fake_images.reshape(fake_images.size(0), 3, 32, 32)\n",
    "    save_image(denorm(fake_images), os.path.join(resul_dir, 'GeneratedSample-{}.png'.format(i)))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9022a0ab266fa41286615ab4189789096e81cf7e07d840c9ac23f04d2f63b2aa"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
