{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from model_init import *\n",
    "from dataset_init import *\n",
    "from utils.others import *\n",
    "from utils.testModel import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Device  cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Current Device \" , device)\n",
    "\n",
    "batch_size = 64\n",
    "num_epochs = 1000\n",
    "num_users = 6\n",
    "nb_classes = 10\n",
    "dataset = 'MNIST'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "clean_dataset, unlearning_dataset, user_wm_dataset, test_dataset = unlearning_dataset_init(dataset, 6)\n",
    "print(len(user_wm_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(unlearning_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "D, G = model_init(dataset, device)\n",
    "D.load_state_dict(torch.load('./D-CL.pth'))\n",
    "optimizerD = torch.optim.Adam(filter(lambda p: p.requires_grad, D.parameters()), 0.0002, betas=(0.5, 0.999))\n",
    "optimizerG = torch.optim.Adam(G.parameters(), 0.0002, betas=(0.5, 0.999))\n",
    "criterion_adv = nn.BCELoss()\n",
    "criterion_aux = nn.CrossEntropyLoss() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing on Users' datasset\n",
      "Testing on User 0 watermark:\n",
      "\n",
      "Test set: Average loss: 0.0001, Accuracy: 967/967 (100%)\n",
      "\n",
      "Testing on User 1 watermark:\n",
      "\n",
      "Test set: Average loss: 0.0001, Accuracy: 1138/1138 (100%)\n",
      "\n",
      "Testing on User 2 watermark:\n",
      "\n",
      "Test set: Average loss: 0.0006, Accuracy: 1003/1003 (100%)\n",
      "\n",
      "Testing on User 3 watermark:\n",
      "\n",
      "Test set: Average loss: 0.0002, Accuracy: 1027/1027 (100%)\n",
      "\n",
      "Testing on User 4 watermark:\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 1000/1000 (100%)\n",
      "\n",
      "Testing on User 5 watermark:\n",
      "\n",
      "Test set: Average loss: 0.0105, Accuracy: 873/873 (100%)\n",
      "\n",
      "Testing on clean test set\n",
      "\n",
      "Test set: Average loss: 0.0689, Accuracy: 9770/10000 (98%)\n",
      "\n",
      "epoch 0\n",
      "Testing on Users' datasset\n",
      "Testing on User 0 watermark:\n",
      "\n",
      "Test set: Average loss: 1.8833, Accuracy: 382/967 (40%)\n",
      "\n",
      "Testing on User 1 watermark:\n",
      "\n",
      "Test set: Average loss: 4.7866, Accuracy: 93/1138 (8%)\n",
      "\n",
      "Testing on User 2 watermark:\n",
      "\n",
      "Test set: Average loss: 2.1425, Accuracy: 235/1003 (23%)\n",
      "\n",
      "Testing on User 3 watermark:\n",
      "\n",
      "Test set: Average loss: 10.8459, Accuracy: 0/1027 (0%)\n",
      "\n",
      "Testing on User 4 watermark:\n",
      "\n",
      "Test set: Average loss: 3.2252, Accuracy: 87/1000 (9%)\n",
      "\n",
      "Testing on User 5 watermark:\n",
      "\n",
      "Test set: Average loss: 0.0084, Accuracy: 872/873 (100%)\n",
      "\n",
      "Testing on clean test set\n",
      "\n",
      "Test set: Average loss: 0.0610, Accuracy: 9808/10000 (98%)\n",
      "\n",
      "epoch 1\n",
      "Testing on Users' datasset\n",
      "Testing on User 0 watermark:\n",
      "\n",
      "Test set: Average loss: 2.2732, Accuracy: 368/967 (38%)\n",
      "\n",
      "Testing on User 1 watermark:\n",
      "\n",
      "Test set: Average loss: 6.2272, Accuracy: 90/1138 (8%)\n",
      "\n",
      "Testing on User 2 watermark:\n",
      "\n",
      "Test set: Average loss: 2.2122, Accuracy: 324/1003 (32%)\n",
      "\n",
      "Testing on User 3 watermark:\n",
      "\n",
      "Test set: Average loss: 10.6719, Accuracy: 0/1027 (0%)\n",
      "\n",
      "Testing on User 4 watermark:\n",
      "\n",
      "Test set: Average loss: 7.5168, Accuracy: 24/1000 (2%)\n",
      "\n",
      "Testing on User 5 watermark:\n",
      "\n",
      "Test set: Average loss: 0.0004, Accuracy: 873/873 (100%)\n",
      "\n",
      "Testing on clean test set\n",
      "\n",
      "Test set: Average loss: 0.0496, Accuracy: 9858/10000 (99%)\n",
      "\n",
      "epoch 2\n",
      "Testing on Users' datasset\n",
      "Testing on User 0 watermark:\n",
      "\n",
      "Test set: Average loss: 4.1208, Accuracy: 176/967 (18%)\n",
      "\n",
      "Testing on User 1 watermark:\n",
      "\n",
      "Test set: Average loss: 7.8238, Accuracy: 66/1138 (6%)\n",
      "\n",
      "Testing on User 2 watermark:\n",
      "\n",
      "Test set: Average loss: 4.0980, Accuracy: 119/1003 (12%)\n",
      "\n",
      "Testing on User 3 watermark:\n",
      "\n",
      "Test set: Average loss: 14.1049, Accuracy: 0/1027 (0%)\n",
      "\n",
      "Testing on User 4 watermark:\n",
      "\n",
      "Test set: Average loss: 14.3540, Accuracy: 0/1000 (0%)\n",
      "\n",
      "Testing on User 5 watermark:\n",
      "\n",
      "Test set: Average loss: 0.0044, Accuracy: 873/873 (100%)\n",
      "\n",
      "Testing on clean test set\n",
      "\n",
      "Test set: Average loss: 0.0608, Accuracy: 9834/10000 (98%)\n",
      "\n",
      "epoch 3\n",
      "Testing on Users' datasset\n",
      "Testing on User 0 watermark:\n",
      "\n",
      "Test set: Average loss: 2.4794, Accuracy: 394/967 (41%)\n",
      "\n",
      "Testing on User 1 watermark:\n",
      "\n",
      "Test set: Average loss: 4.6911, Accuracy: 197/1138 (17%)\n",
      "\n",
      "Testing on User 2 watermark:\n",
      "\n",
      "Test set: Average loss: 0.6443, Accuracy: 725/1003 (72%)\n",
      "\n",
      "Testing on User 3 watermark:\n",
      "\n",
      "Test set: Average loss: 8.6777, Accuracy: 1/1027 (0%)\n",
      "\n",
      "Testing on User 4 watermark:\n",
      "\n",
      "Test set: Average loss: 7.4169, Accuracy: 1/1000 (0%)\n",
      "\n",
      "Testing on User 5 watermark:\n",
      "\n",
      "Test set: Average loss: 0.0004, Accuracy: 873/873 (100%)\n",
      "\n",
      "Testing on clean test set\n",
      "\n",
      "Test set: Average loss: 0.0471, Accuracy: 9838/10000 (98%)\n",
      "\n",
      "epoch 4\n",
      "Testing on Users' datasset\n",
      "Testing on User 0 watermark:\n",
      "\n",
      "Test set: Average loss: 4.9261, Accuracy: 208/967 (22%)\n",
      "\n",
      "Testing on User 1 watermark:\n",
      "\n",
      "Test set: Average loss: 4.0027, Accuracy: 195/1138 (17%)\n",
      "\n",
      "Testing on User 2 watermark:\n",
      "\n",
      "Test set: Average loss: 4.2296, Accuracy: 137/1003 (14%)\n",
      "\n",
      "Testing on User 3 watermark:\n",
      "\n",
      "Test set: Average loss: 11.4059, Accuracy: 1/1027 (0%)\n",
      "\n",
      "Testing on User 4 watermark:\n",
      "\n",
      "Test set: Average loss: 10.3600, Accuracy: 0/1000 (0%)\n",
      "\n",
      "Testing on User 5 watermark:\n",
      "\n",
      "Test set: Average loss: 0.0003, Accuracy: 873/873 (100%)\n",
      "\n",
      "Testing on clean test set\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/zcy/MODA/unlearning.ipynb Cell 6\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bregion-8.seetacloud.com/home/zcy/MODA/unlearning.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m     real_score \u001b[39m=\u001b[39m predictR\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bregion-8.seetacloud.com/home/zcy/MODA/unlearning.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mepoch\u001b[39m\u001b[39m\"\u001b[39m,epoch)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bregion-8.seetacloud.com/home/zcy/MODA/unlearning.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m comprehensive_user_test(D, device, test_loader, user_wm_dataset)\n",
      "File \u001b[0;32m/home/zcy/MODA/utils/testModel.py:54\u001b[0m, in \u001b[0;36mcomprehensive_user_test\u001b[0;34m(model, device, clean_loader, user_wm_dataset)\u001b[0m\n\u001b[1;32m     51\u001b[0m     test_list\u001b[39m.\u001b[39mappend(a)\n\u001b[1;32m     53\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mTesting on clean test set\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 54\u001b[0m a \u001b[39m=\u001b[39m test(model, device, clean_loader)\n\u001b[1;32m     55\u001b[0m test_list\u001b[39m.\u001b[39mappend(a)\n\u001b[1;32m     56\u001b[0m \u001b[39mreturn\u001b[39;00m test_list\n",
      "File \u001b[0;32m/home/zcy/MODA/utils/testModel.py:22\u001b[0m, in \u001b[0;36mtest\u001b[0;34m(model, device, test_loader)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[39m# for acGAN discriminator output two vector\u001b[39;00m\n\u001b[1;32m     21\u001b[0m _, output \u001b[39m=\u001b[39m model(data)\n\u001b[0;32m---> 22\u001b[0m test_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m F\u001b[39m.\u001b[39;49mcross_entropy(output, target, reduction\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39msum\u001b[39;49m\u001b[39m'\u001b[39;49m)\u001b[39m.\u001b[39;49mitem()  \u001b[39m# sum up batch loss\u001b[39;00m\n\u001b[1;32m     23\u001b[0m pred \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39margmax(dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, keepdim\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)  \u001b[39m# get the index of the max log-probability\u001b[39;00m\n\u001b[1;32m     24\u001b[0m correct \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m pred\u001b[39m.\u001b[39meq(target\u001b[39m.\u001b[39mview_as(pred))\u001b[39m.\u001b[39msum()\u001b[39m.\u001b[39mitem()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "total_step = len(train_loader)\n",
    "comprehensive_user_test(D, device, test_loader, user_wm_dataset)\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_idx, (images, target) in enumerate(train_loader):\n",
    "        images, target = images.to(device), torch.LongTensor(target).to(device)\n",
    "        predictR, predictRLabel = D(images) #image from the real dataset\n",
    "        loss_real_aux = criterion_aux(predictRLabel, target)\n",
    "        optimizerD.zero_grad()\n",
    "        optimizerG.zero_grad()\n",
    "        loss_real_aux.backward()\n",
    "        optimizerD.step()\n",
    "        real_score = predictR\n",
    "    print(\"epoch\",epoch)\n",
    "    comprehensive_user_test(D, device, test_loader, user_wm_dataset)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9022a0ab266fa41286615ab4189789096e81cf7e07d840c9ac23f04d2f63b2aa"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
